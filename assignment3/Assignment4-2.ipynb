{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset \n",
    "a) Download the first 6 classes (class 0 to 5) of \"German Traffic Sign Recognition Benchmark\" dataset from Kaggle\n",
    "https://www.kaggle.com/datasets/aakcodebreaker/german-traffic-sign-recognition-benchmark . Copy both training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Calculate the mean and standard deviation of the training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate mean and standard deviation of training images\nimport os\nfrom PIL import Image\n\ndef calculate_mean_std(data_dir, classes):\n    \"\"\"\n    Calculate mean and std deviation for the training dataset\n    data_dir: path to the Train folder\n    classes: list of class folders to include (0-5)\n    \"\"\"\n    pixel_sum = np.zeros(3)\n    pixel_sq_sum = np.zeros(3)\n    pixel_count = 0\n    \n    for class_id in classes:\n        class_path = os.path.join(data_dir, str(class_id))\n        if not os.path.exists(class_path):\n            continue\n        \n        for img_name in os.listdir(class_path):\n            img_path = os.path.join(class_path, img_name)\n            try:\n                img = Image.open(img_path).convert('RGB')\n                img_array = np.array(img) / 255.0  # Normalize to [0, 1]\n                \n                pixel_sum += img_array.sum(axis=(0, 1))\n                pixel_sq_sum += (img_array ** 2).sum(axis=(0, 1))\n                pixel_count += img_array.shape[0] * img_array.shape[1]\n            except:\n                continue\n    \n    mean = pixel_sum / pixel_count\n    std_dev = np.sqrt(pixel_sq_sum / pixel_count - mean ** 2)\n    \n    return mean, std_dev\n\n# Assuming the dataset is in a folder structure like:\n# ./data/Train/0/, ./data/Train/1/, ..., ./data/Train/5/\n# Modify this path according to where you extract the dataset\ntrain_data_path = './data/Train'\nclasses = [0, 1, 2, 3, 4, 5]\n\n# Check if data directory exists\nif os.path.exists(train_data_path):\n    mean, std_dev = calculate_mean_std(train_data_path, classes)\n    print(f\"Mean: {mean}\")\n    print(f\"Standard Deviation: {std_dev}\")\nelse:\n    print(\"Dataset not found. Please download and extract the dataset to './data/'\")\n    print(\"Using ImageNet statistics as placeholder:\")\n    # Using ImageNet statistics as fallback\n    mean = np.array([0.485, 0.456, 0.406])\n    std_dev = np.array([0.229, 0.224, 0.225])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Implement the custom dataset class for training and test.  Don't use Pytorch ``ImageFolder``. \n",
    "For training, do the following transforms and data augmentation: \n",
    " - resize the images to 64 by 64.\n",
    " - Horizontal flip with a probabilty of 0.25\n",
    " - Vertical flip with a probabilty of 0.25\n",
    " - random rotation between 0 and 10 degrees.\n",
    " - Convert ``dtype`` to ``float32`` and scale between $0$ and $1$\n",
    " - then Normalize with the mean and std. dev. obtained in step b.\n",
    " \n",
    "For testing, resize to 64 by 64, convert ``dtype`` to ``float32`` and scale between $0$ and $1$ and normalize.\n",
    "\n",
    "(i) In the init method, store the path to the images and labels. You can use any data structure of your choice, e.g., list/nested list, dictionary, or pandas dataframe.\n",
    "\n",
    "(ii) in the getitem method, use the path stored at position ``idx`` to load the image. Return image and its label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class TrafficSignDataset(Dataset):\n    def __init__(self, root_dir, training=True, transform=None, mean=None, std_dev=None):\n        \"\"\"\n        Custom Dataset for German Traffic Sign Recognition\n        root_dir: path to Train or Test folder\n        training: True for training mode (with augmentation), False for test mode\n        transform: optional additional transforms\n        mean, std_dev: normalization parameters\n        \"\"\"\n        self.root_dir = root_dir\n        self.training = training\n        self.transform = transform\n        self.mean = mean if mean is not None else np.array([0.485, 0.456, 0.406])\n        self.std_dev = std_dev if std_dev is not None else np.array([0.229, 0.224, 0.225])\n        \n        # Store image paths and labels\n        self.image_paths = []\n        self.labels = []\n        \n        # Load data for classes 0-5\n        for class_id in range(6):\n            class_path = os.path.join(root_dir, str(class_id))\n            if not os.path.exists(class_path):\n                continue\n            \n            for img_name in os.listdir(class_path):\n                img_path = os.path.join(class_path, img_name)\n                self.image_paths.append(img_path)\n                self.labels.append(class_id)\n        \n        # Define transforms based on training/test mode\n        if self.training:\n            # Training transforms with augmentation\n            self.transforms_list = transforms.Compose([\n                transforms.Resize((64, 64)),\n                transforms.RandomHorizontalFlip(p=0.25),\n                transforms.RandomVerticalFlip(p=0.25),\n                transforms.RandomRotation(degrees=10),\n                transforms.ToTensor(),  # Converts to float32 and scales to [0, 1]\n                transforms.Normalize(mean=self.mean, std=self.std_dev)\n            ])\n        else:\n            # Test transforms without augmentation\n            self.transforms_list = transforms.Compose([\n                transforms.Resize((64, 64)),\n                transforms.ToTensor(),  # Converts to float32 and scales to [0, 1]\n                transforms.Normalize(mean=self.mean, std=self.std_dev)\n            ])\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        # Load image\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert('RGB')\n        \n        # Apply transforms\n        image = self.transforms_list(image)\n        \n        # Get label\n        label = self.labels[idx]\n        \n        return image, label"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Instantiate training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Instantiate training and test datasets\ntrain_data_path = './data/Train'\ntest_data_path = './data/Test'\n\n# Create full training dataset\nfull_train_dataset = TrafficSignDataset(\n    root_dir=train_data_path,\n    training=True,\n    mean=mean,\n    std_dev=std_dev\n)\n\n# Create test dataset\ntest_dataset = TrafficSignDataset(\n    root_dir=test_data_path,\n    training=False,\n    mean=mean,\n    std_dev=std_dev\n)\n\nprint(f\"Full training dataset size: {len(full_train_dataset)}\")\nprint(f\"Test dataset size: {len(test_dataset)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Partition the training dataset further into training and validation. 20% of training images are validation. Configure the dataloaders with batch size of 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Partition training dataset into train and validation (80/20 split)\nfrom torch.utils.data import random_split\n\n# Calculate split sizes\ntotal_size = len(full_train_dataset)\ntrain_size = int(0.8 * total_size)\nval_size = total_size - train_size\n\n# Split the dataset\ntrain_dataset, val_dataset = random_split(\n    full_train_dataset, \n    [train_size, val_size],\n    generator=torch.Generator().manual_seed(42)  # For reproducibility\n)\n\nprint(f\"Training set size: {len(train_dataset)}\")\nprint(f\"Validation set size: {len(val_dataset)}\")\nprint(f\"Test set size: {len(test_dataset)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure dataloaders with batch size of 16\nbatch_size = 16\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=2\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=batch_size,\n    shuffle=False,\n    num_workers=2\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=batch_size,\n    shuffle=False,\n    num_workers=2\n)\n\nprint(f\"Number of training batches: {len(train_loader)}\")\nprint(f\"Number of validation batches: {len(val_loader)}\")\nprint(f\"Number of test batches: {len(test_loader)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Visualize two random training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize two random training images\nimport random\n\n# Get two random indices\nindices = random.sample(range(len(train_dataset)), 2)\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\nfor idx, ax in zip(indices, axes):\n    # Get image and label\n    image, label = train_dataset[idx]\n    \n    # Denormalize image for visualization\n    image_np = image.numpy().transpose(1, 2, 0)\n    image_np = image_np * std_dev + mean\n    image_np = np.clip(image_np, 0, 1)\n    \n    # Display\n    ax.imshow(image_np)\n    ax.set_title(f'Class: {label}')\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. CNN\n",
    "a) Build a NN network specified in the figure below and initialize the parameters. \n",
    "\n",
    "The network consists of:\n",
    "1. Convolution layer: 16 3x3 filters with stride=1. Set the padding such that the output size is half of the input. Convolution is followed by BatchNorm and ReLu Activation fuction.\n",
    "1. Maxpool with 2x2 filters and stride = 2\n",
    "2. Convolution layer: 16 3x3 filters with stride=2. Set the padding such that the output size is half of the input. Convolution is followed by BatchNorm and ReLu Activation fuction.\n",
    "1. Convolution layer: 32 3x3 filters. Convolution is followed by BatchNorm and ReLu Activation fuction.\n",
    "2. Convolution layer: 64 3x3 filters with stride=2. Set the padding such that the output size is half of the input. Convolution is followed by BatchNorm and ReLu Activation fuction.\n",
    "3. A fully connected output layer.\n",
    "\n",
    "\n",
    "Note: select padding of the convolution input such that output height and width are an integer division of input size. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        \n        # Layer 1: Conv (16, 3x3, stride=1, padding=1) + BatchNorm + ReLU\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n        self.bn1 = nn.BatchNorm2d(16)\n        self.relu1 = nn.ReLU()\n        \n        # MaxPool (2x2, stride=2)\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        # Layer 2: Conv (16, 3x3, stride=2, padding=1) + BatchNorm + ReLU\n        self.conv2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=2, padding=1)\n        self.bn2 = nn.BatchNorm2d(16)\n        self.relu2 = nn.ReLU()\n        \n        # Layer 3: Conv (32, 3x3, stride=1, padding=1) + BatchNorm + ReLU\n        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n        self.bn3 = nn.BatchNorm2d(32)\n        self.relu3 = nn.ReLU()\n        \n        # Layer 4: Conv (64, 3x3, stride=2, padding=1) + BatchNorm + ReLU\n        self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1)\n        self.bn4 = nn.BatchNorm2d(64)\n        self.relu4 = nn.ReLU()\n        \n        # Fully connected output layer\n        # After all convolutions: 64x64 -> 64x64 -> 32x32 -> 16x16 -> 16x16 -> 8x8\n        self.fc = nn.Linear(64 * 8 * 8, 6)  # 6 classes (0-5)\n        \n    def forward(self, input):\n        # Layer 1: Conv + BN + ReLU\n        x = self.conv1(input)\n        x = self.bn1(x)\n        x = self.relu1(x)\n        \n        # MaxPool\n        x = self.maxpool(x)\n        \n        # Layer 2: Conv + BN + ReLU\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu2(x)\n        \n        # Layer 3: Conv + BN + ReLU\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.relu3(x)\n        \n        # Layer 4: Conv + BN + ReLU\n        x = self.conv4(x)\n        x = self.bn4(x)\n        x = self.relu4(x)\n        \n        # Flatten\n        x = x.view(x.size(0), -1)\n        \n        # Fully connected layer\n        output = self.fc(x)\n        \n        return output"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CNN()\n",
    "\n",
    "# if you use GPU, uncomment the floolowing line\n",
    "# cnn_model = cnn_model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Do forward propagation for a minibatch and verify that the output shape equals the number of samples in a minibatch by the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,labels = next(iter(train_loader))\n",
    "\n",
    "output = cnn_model(data)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Set up the loss and optimizer. 8$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Loss function\nloss_fn = nn.CrossEntropyLoss()\n\n# Hyperparameters (will be set after grid search)\n# These are placeholder values - actual values will be determined by grid search\nlr = 0.01\nmomentum = 0.9\nweight_decay = 1e-4\nepochs = 50\n\n# Optimizer (will be properly configured after hyperparameter tuning)\n# optimizer = optim.SGD(cnn_model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Hyperparamter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Do grid search to find a good combination of the optimizer hyperparamters. Use \n",
    "1. lr=1e-3, 1e-2, 1e-1, 1e-0, 10\n",
    "1. momentum = 0.85, 0.9, 0.95, 0.99\n",
    "2. weight decay = 0, 1e-4, 1e-2\n",
    "\n",
    "You need to use nested loop. Run each experiment for two epochs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Hyperparameter grid search\nimport copy\nfrom tqdm import tqdm\n\n# Define hyperparameter grid\nlearning_rates = [1e-3, 1e-2, 1e-1, 1.0, 10.0]\nmomentums = [0.85, 0.9, 0.95, 0.99]\nweight_decays = [0, 1e-4, 1e-2]\n\n# Store results\ngrid_search_results = []\ngrid_search_losses = {}\n\nprint(\"Starting Grid Search...\")\nprint(f\"Total combinations: {len(learning_rates) * len(momentums) * len(weight_decays)}\")\n\n# Grid search with nested loops\nfor lr_val in learning_rates:\n    for momentum_val in momentums:\n        for wd_val in weight_decays:\n            print(f\"\\nTesting: lr={lr_val}, momentum={momentum_val}, weight_decay={wd_val}\")\n            \n            # Create a fresh model for each configuration\n            model = CNN()\n            # Uncomment if using GPU\n            # model = model.cuda()\n            \n            # Setup optimizer with current hyperparameters\n            optimizer = optim.SGD(model.parameters(), lr=lr_val, momentum=momentum_val, weight_decay=wd_val)\n            \n            # Train for 2 epochs\n            epoch_losses = []\n            for epoch in range(2):\n                model.train()\n                running_loss = 0.0\n                num_batches = 0\n                \n                for batch in train_loader:\n                    optimizer.zero_grad()\n                    data, target_labels = batch\n                    # Uncomment if using GPU\n                    # data, target_labels = data.cuda(), target_labels.cuda()\n                    \n                    # Forward pass\n                    prediction = model(data)\n                    loss = loss_fn(prediction, target_labels)\n                    \n                    # Backward pass\n                    loss.backward()\n                    optimizer.step()\n                    \n                    running_loss += loss.item()\n                    num_batches += 1\n                \n                avg_loss = running_loss / num_batches\n                epoch_losses.append(avg_loss)\n                print(f\"  Epoch {epoch+1}/2, Loss: {avg_loss:.4f}\")\n            \n            # Store results\n            config_key = f\"lr={lr_val}_m={momentum_val}_wd={wd_val}\"\n            grid_search_losses[config_key] = epoch_losses\n            grid_search_results.append({\n                'lr': lr_val,\n                'momentum': momentum_val,\n                'weight_decay': wd_val,\n                'final_loss': epoch_losses[-1],\n                'losses': epoch_losses\n            })\n\n# Find best configuration\nbest_config = min(grid_search_results, key=lambda x: x['final_loss'])\nprint(\"\\n\" + \"=\"*60)\nprint(\"Best configuration found:\")\nprint(f\"  Learning rate: {best_config['lr']}\")\nprint(f\"  Momentum: {best_config['momentum']}\")\nprint(f\"  Weight decay: {best_config['weight_decay']}\")\nprint(f\"  Final loss: {best_config['final_loss']:.4f}\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Plot the training loss. Label each line with corresponding parameters (lr, momentum, and weight decay).  Pick the set of parameters that yields the best loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot training loss for all hyperparameter combinations\nplt.figure(figsize=(15, 10))\n\nfor config_key, losses in grid_search_losses.items():\n    plt.plot([1, 2], losses, marker='o', label=config_key)\n\nplt.xlabel('Epoch')\nplt.ylabel('Training Loss')\nplt.title('Hyperparameter Grid Search - Training Loss Comparison')\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Print top 5 configurations\nprint(\"\\nTop 5 configurations by final loss:\")\nsorted_results = sorted(grid_search_results, key=lambda x: x['final_loss'])\nfor i, config in enumerate(sorted_results[:5]):\n    print(f\"{i+1}. lr={config['lr']}, momentum={config['momentum']}, \"\n          f\"weight_decay={config['weight_decay']}, final_loss={config['final_loss']:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Use the hyperparamters selected in the previous stage to train the model for 50 epochs. Track validation accuracy and save the model whenever validation accuracy is higher. Follow the template and instructions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a new model with best hyperparameters\nfinal_cnn_model = CNN()\n# Uncomment if using GPU\n# final_cnn_model = final_cnn_model.cuda()\n\n# Setup optimizer with best hyperparameters from grid search\nbest_lr = best_config['lr']\nbest_momentum = best_config['momentum']\nbest_weight_decay = best_config['weight_decay']\n\noptimizer = optim.SGD(final_cnn_model.parameters(), lr=best_lr, momentum=best_momentum, weight_decay=best_weight_decay)\n\nprint(f\"Training with: lr={best_lr}, momentum={best_momentum}, weight_decay={best_weight_decay}\")\n\n# Training tracking\ntraining_avg_loss = []\nval_avg_loss = []\nbest_val_acc = 0.0\ntrain_acc = []\nval_acc = []\n\nepochs = 50\n\nfor epoch in range(epochs):\n    # Training\n    final_cnn_model.train()\n    \n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for batch in train_loader:\n        # Zero gradients\n        optimizer.zero_grad()\n        \n        # Unpack batch\n        data, target_labels = batch\n        # Uncomment if using GPU\n        # data, target_labels = data.cuda(), target_labels.cuda()\n        \n        # Forward propagation\n        prediction = final_cnn_model(data)\n        \n        # Calculate loss\n        loss = loss_fn(prediction, target_labels)\n        \n        # Backward propagation\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n        \n        # Track training loss\n        running_loss += loss.item()\n        \n        # Calculate training accuracy\n        _, predicted = torch.max(prediction.data, 1)\n        total += target_labels.size(0)\n        correct += (predicted == target_labels).sum().item()\n    \n    # Calculate average training loss and accuracy\n    avg_train_loss = running_loss / len(train_loader)\n    training_avg_loss.append(avg_train_loss)\n    train_accuracy = 100 * correct / total\n    train_acc.append(train_accuracy)\n    \n    ###### Validation ######\n    final_cnn_model.eval()\n    \n    val_running_loss = 0.0\n    val_correct = 0\n    val_total = 0\n    \n    with torch.no_grad():\n        for batch in val_loader:\n            # Unpack batch\n            data, target_labels = batch\n            # Uncomment if using GPU\n            # data, target_labels = data.cuda(), target_labels.cuda()\n            \n            # Forward propagation\n            prediction = final_cnn_model(data)\n            \n            # Calculate loss\n            loss = loss_fn(prediction, target_labels)\n            \n            # Track validation loss\n            val_running_loss += loss.item()\n            \n            # Calculate validation accuracy\n            _, predicted = torch.max(prediction.data, 1)\n            val_total += target_labels.size(0)\n            val_correct += (predicted == target_labels).sum().item()\n    \n    # Calculate average validation loss and accuracy\n    avg_val_loss = val_running_loss / len(val_loader)\n    val_avg_loss.append(avg_val_loss)\n    val_accuracy = 100 * val_correct / val_total\n    val_acc.append(val_accuracy)\n    \n    # Print progress\n    print(f\"Epoch [{epoch+1}/{epochs}] - \"\n          f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}% - \"\n          f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n    \n    # Save model if validation accuracy improved\n    if val_accuracy > best_val_acc:\n        best_val_acc = val_accuracy\n        torch.save(final_cnn_model.state_dict(), 'best_cnn_model.pth')\n        print(f\"  --> Model saved! New best validation accuracy: {best_val_acc:.2f}%\")\n\nprint(f\"\\nTraining complete! Best validation accuracy: {best_val_acc:.2f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Plot losses vs epoch for training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot losses vs epoch for training and validation datasets\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n\n# Plot losses\nax1.plot(range(1, epochs+1), training_avg_loss, label='Training Loss', marker='o', markersize=3)\nax1.plot(range(1, epochs+1), val_avg_loss, label='Validation Loss', marker='s', markersize=3)\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss')\nax1.set_title('Training and Validation Loss')\nax1.legend()\nax1.grid(True)\n\n# Plot accuracies\nax2.plot(range(1, epochs+1), train_acc, label='Training Accuracy', marker='o', markersize=3)\nax2.plot(range(1, epochs+1), val_acc, label='Validation Accuracy', marker='s', markersize=3)\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Accuracy (%)')\nax2.set_title('Training and Validation Accuracy')\nax2.legend()\nax2.grid(True)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Load the saved best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the saved best model\nloaded_model = CNN()\nloaded_model.load_state_dict(torch.load('best_cnn_model.pth'))\n# Uncomment if using GPU\n# loaded_model = loaded_model.cuda()\nloaded_model.eval()\n\nprint(\"Best model loaded successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Use the loaded model to calculate the accuracy for the test data. Show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate the loaded model on test data\nloaded_model.eval()\n\ntest_correct = 0\ntest_total = 0\ntest_running_loss = 0.0\n\nwith torch.no_grad():\n    for batch in test_loader:\n        data, target_labels = batch\n        # Uncomment if using GPU\n        # data, target_labels = data.cuda(), target_labels.cuda()\n        \n        # Forward propagation\n        prediction = loaded_model(data)\n        \n        # Calculate loss\n        loss = loss_fn(prediction, target_labels)\n        test_running_loss += loss.item()\n        \n        # Calculate accuracy\n        _, predicted = torch.max(prediction.data, 1)\n        test_total += target_labels.size(0)\n        test_correct += (predicted == target_labels).sum().item()\n\n# Calculate final metrics\ntest_accuracy = 100 * test_correct / test_total\ntest_loss = test_running_loss / len(test_loader)\n\nprint(\"=\"*60)\nprint(\"TEST SET RESULTS (Best CNN Model)\")\nprint(\"=\"*60)\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.2f}%\")\nprint(f\"Correct predictions: {test_correct}/{test_total}\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Use transfer learning to finetune a pretrained ``ResNet18`` model. \n",
    "\n",
    "1. Load the pretrained model\n",
    "2. Get the transform for training and validation.\n",
    "3. Define the dataset for training, validation, and test again with the new transform.\n",
    "4. Define dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load pretrained ResNet18 model\nfrom torchvision.models import resnet18, ResNet18_Weights\n\n# Load pretrained ResNet18 with latest weights\nweights = ResNet18_Weights.DEFAULT\nresnet_model = resnet18(weights=weights)\n\n# Get the transforms for pretrained ResNet18\nresnet_transforms_train = weights.transforms()\nresnet_transforms_val = weights.transforms()\n\nprint(f\"Loaded pretrained ResNet18\")\nprint(f\"Using transforms: {resnet_transforms_train}\")\n\n# Create new datasets with ResNet18 transforms\n# For ResNet, we need to use the standard ImageNet preprocessing\nfrom torchvision import transforms as T\n\n# Define custom transforms that include data augmentation for training\nresnet_train_transforms = T.Compose([\n    T.Resize((64, 64)),\n    T.RandomHorizontalFlip(p=0.25),\n    T.RandomVerticalFlip(p=0.25),\n    T.RandomRotation(degrees=10),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet statistics\n])\n\nresnet_test_transforms = T.Compose([\n    T.Resize((64, 64)),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet statistics\n])\n\n# Create datasets with ResNet transforms\nclass ResNetTrafficSignDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.image_paths = []\n        self.labels = []\n        \n        # Load data for classes 0-5\n        for class_id in range(6):\n            class_path = os.path.join(root_dir, str(class_id))\n            if not os.path.exists(class_path):\n                continue\n            \n            for img_name in os.listdir(class_path):\n                img_path = os.path.join(class_path, img_name)\n                self.image_paths.append(img_path)\n                self.labels.append(class_id)\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        label = self.labels[idx]\n        return image, label\n\n# Create ResNet datasets\nresnet_full_train_dataset = ResNetTrafficSignDataset(\n    root_dir=train_data_path,\n    transform=resnet_train_transforms\n)\n\nresnet_test_dataset = ResNetTrafficSignDataset(\n    root_dir=test_data_path,\n    transform=resnet_test_transforms\n)\n\n# Split into train and validation\nresnet_train_size = int(0.8 * len(resnet_full_train_dataset))\nresnet_val_size = len(resnet_full_train_dataset) - resnet_train_size\n\nresnet_train_dataset, resnet_val_dataset = random_split(\n    resnet_full_train_dataset,\n    [resnet_train_size, resnet_val_size],\n    generator=torch.Generator().manual_seed(42)\n)\n\n# Create dataloaders\nresnet_train_loader = DataLoader(resnet_train_dataset, batch_size=16, shuffle=True, num_workers=2)\nresnet_val_loader = DataLoader(resnet_val_dataset, batch_size=16, shuffle=False, num_workers=2)\nresnet_test_loader = DataLoader(resnet_test_dataset, batch_size=16, shuffle=False, num_workers=2)\n\nprint(f\"\\nResNet datasets created:\")\nprint(f\"  Training: {len(resnet_train_dataset)}\")\nprint(f\"  Validation: {len(resnet_val_dataset)}\")\nprint(f\"  Test: {len(resnet_test_dataset)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) \n",
    "1. Set the ``requires_grad`` to ``False`` for all parameters of the network.\n",
    "2. Replace the last layer to adpat to your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Freeze all parameters\nfor param in resnet_model.parameters():\n    param.requires_grad = False\n\n# Replace the last fully connected layer\n# ResNet18 has 512 features in the final layer\nnum_features = resnet_model.fc.in_features\nresnet_model.fc = nn.Linear(num_features, 6)  # 6 classes for our traffic signs\n\n# Verify that only the final layer is trainable\nprint(\"Trainable parameters:\")\nfor name, param in resnet_model.named_parameters():\n    if param.requires_grad:\n        print(f\"  {name}: {param.shape}\")\n\n# Uncomment if using GPU\n# resnet_model = resnet_model.cuda()\n\nprint(f\"\\nResNet18 modified: Final layer has {num_features} -> 6 outputs\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Train and evaluate with a suitable set of hyperparameters and save the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train and evaluate ResNet18 with transfer learning\n\n# Setup for transfer learning\nresnet_loss_fn = nn.CrossEntropyLoss()\nresnet_optimizer = optim.Adam(resnet_model.fc.parameters(), lr=0.001)  # Only train the final layer\nresnet_epochs = 20\n\n# Tracking\nresnet_training_losses = []\nresnet_val_losses = []\nresnet_train_accs = []\nresnet_val_accs = []\nbest_resnet_val_acc = 0.0\n\nprint(f\"Training ResNet18 for {resnet_epochs} epochs...\")\nprint(\"=\"*60)\n\nfor epoch in range(resnet_epochs):\n    # Training\n    resnet_model.train()\n    \n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for batch in resnet_train_loader:\n        resnet_optimizer.zero_grad()\n        \n        data, target_labels = batch\n        # Uncomment if using GPU\n        # data, target_labels = data.cuda(), target_labels.cuda()\n        \n        prediction = resnet_model(data)\n        loss = resnet_loss_fn(prediction, target_labels)\n        \n        loss.backward()\n        resnet_optimizer.step()\n        \n        running_loss += loss.item()\n        _, predicted = torch.max(prediction.data, 1)\n        total += target_labels.size(0)\n        correct += (predicted == target_labels).sum().item()\n    \n    avg_train_loss = running_loss / len(resnet_train_loader)\n    train_accuracy = 100 * correct / total\n    resnet_training_losses.append(avg_train_loss)\n    resnet_train_accs.append(train_accuracy)\n    \n    # Validation\n    resnet_model.eval()\n    \n    val_running_loss = 0.0\n    val_correct = 0\n    val_total = 0\n    \n    with torch.no_grad():\n        for batch in resnet_val_loader:\n            data, target_labels = batch\n            # Uncomment if using GPU\n            # data, target_labels = data.cuda(), target_labels.cuda()\n            \n            prediction = resnet_model(data)\n            loss = resnet_loss_fn(prediction, target_labels)\n            \n            val_running_loss += loss.item()\n            _, predicted = torch.max(prediction.data, 1)\n            val_total += target_labels.size(0)\n            val_correct += (predicted == target_labels).sum().item()\n    \n    avg_val_loss = val_running_loss / len(resnet_val_loader)\n    val_accuracy = 100 * val_correct / val_total\n    resnet_val_losses.append(avg_val_loss)\n    resnet_val_accs.append(val_accuracy)\n    \n    print(f\"Epoch [{epoch+1}/{resnet_epochs}] - \"\n          f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}% - \"\n          f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n    \n    # Save best model\n    if val_accuracy > best_resnet_val_acc:\n        best_resnet_val_acc = val_accuracy\n        torch.save(resnet_model.state_dict(), 'best_resnet_model.pth')\n        print(f\"  --> ResNet model saved! New best validation accuracy: {best_resnet_val_acc:.2f}%\")\n\nprint(\"=\"*60)\nprint(f\"ResNet Training complete! Best validation accuracy: {best_resnet_val_acc:.2f}%\")\n\n# Plot ResNet training results\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n\nax1.plot(range(1, resnet_epochs+1), resnet_training_losses, label='Training Loss', marker='o', markersize=3)\nax1.plot(range(1, resnet_epochs+1), resnet_val_losses, label='Validation Loss', marker='s', markersize=3)\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss')\nax1.set_title('ResNet18 - Training and Validation Loss')\nax1.legend()\nax1.grid(True)\n\nax2.plot(range(1, resnet_epochs+1), resnet_train_accs, label='Training Accuracy', marker='o', markersize=3)\nax2.plot(range(1, resnet_epochs+1), resnet_val_accs, label='Validation Accuracy', marker='s', markersize=3)\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Accuracy (%)')\nax2.set_title('ResNet18 - Training and Validation Accuracy')\nax2.legend()\nax2.grid(True)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) evaluate on the test data and report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load best ResNet model and evaluate on test data\nbest_resnet_model = resnet18(weights=None)\nnum_features = best_resnet_model.fc.in_features\nbest_resnet_model.fc = nn.Linear(num_features, 6)\nbest_resnet_model.load_state_dict(torch.load('best_resnet_model.pth'))\n# Uncomment if using GPU\n# best_resnet_model = best_resnet_model.cuda()\nbest_resnet_model.eval()\n\n# Evaluate on test set\ntest_correct = 0\ntest_total = 0\ntest_running_loss = 0.0\n\nwith torch.no_grad():\n    for batch in resnet_test_loader:\n        data, target_labels = batch\n        # Uncomment if using GPU\n        # data, target_labels = data.cuda(), target_labels.cuda()\n        \n        prediction = best_resnet_model(data)\n        loss = resnet_loss_fn(prediction, target_labels)\n        test_running_loss += loss.item()\n        \n        _, predicted = torch.max(prediction.data, 1)\n        test_total += target_labels.size(0)\n        test_correct += (predicted == target_labels).sum().item()\n\ntest_accuracy = 100 * test_correct / test_total\ntest_loss = test_running_loss / len(resnet_test_loader)\n\nprint(\"=\"*60)\nprint(\"TEST SET RESULTS (ResNet18 Transfer Learning)\")\nprint(\"=\"*60)\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.2f}%\")\nprint(f\"Correct predictions: {test_correct}/{test_total}\")\nprint(\"=\"*60)\n\n# Compare with custom CNN results\nprint(\"\\nCOMPARISON:\")\nprint(\"=\"*60)\nprint(\"The ResNet18 model with transfer learning typically achieves\")\nprint(\"better performance compared to the custom CNN trained from scratch,\")\nprint(\"especially when the training dataset is limited.\")\nprint(\"=\"*60)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}