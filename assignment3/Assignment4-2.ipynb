{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset \n",
    "a) Download the first 6 classes (class 0 to 5) of \"German Traffic Sign Recognition Benchmark\" dataset from Kaggle\n",
    "https://www.kaggle.com/datasets/aakcodebreaker/german-traffic-sign-recognition-benchmark . Copy both training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Calculate the mean and standard deviation of the training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "\n",
    "\n",
    "mean = None\n",
    "std_dev = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Implement the custom dataset class for training and test.  Don't use Pytorch ``ImageFolder``. \n",
    "For training, do the following transforms and data augmentation: \n",
    " - resize the images to 64 by 64.\n",
    " - Horizontal flip with a probabilty of 0.25\n",
    " - Vertical flip with a probabilty of 0.25\n",
    " - random rotation between 0 and 10 degrees.\n",
    " - Convert ``dtype`` to ``float32`` and scale between $0$ and $1$\n",
    " - then Normalize with the mean and std. dev. obtained in step b.\n",
    " \n",
    "For testing, resize to 64 by 64, convert ``dtype`` to ``float32`` and scale between $0$ and $1$ and normalize.\n",
    "\n",
    "(i) In the init method, store the path to the images and labels. You can use any data structure of your choice, e.g., list/nested list, dictionary, or pandas dataframe.\n",
    "\n",
    "(ii) in the getitem method, use the path stored at position ``idx`` to load the image. Return image and its label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HousingDataset(Dataset):\n",
    "    def __init__(self, root_dir, training = True, transform=None): # Here root_dir is the path to train or test folder.\n",
    "        # coomplete this method as per instructions\n",
    "        \n",
    "    def __len__(self):\n",
    "        length = None\n",
    "        return length\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #here load image and do the transformatoin\n",
    "        image = None\n",
    "        label_price = None\n",
    "        return image, label_price\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Instantiate training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Partition the training dataset further into training and validation. 20% of training images are validation. Configure the dataloaders with batch size of 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Visualize two random training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. CNN\n",
    "a) Build a NN network specified in the figure below and initialize the parameters. \n",
    "\n",
    "The network consists of:\n",
    "1. Convolution layer: 16 3x3 filters with stride=1. Set the padding such that the output size is half of the input. Convolution is followed by BatchNorm and ReLu Activation fuction.\n",
    "1. Maxpool with 2x2 filters and stride = 2\n",
    "2. Convolution layer: 16 3x3 filters with stride=2. Set the padding such that the output size is half of the input. Convolution is followed by BatchNorm and ReLu Activation fuction.\n",
    "1. Convolution layer: 32 3x3 filters. Convolution is followed by BatchNorm and ReLu Activation fuction.\n",
    "2. Convolution layer: 64 3x3 filters with stride=2. Set the padding such that the output size is half of the input. Convolution is followed by BatchNorm and ReLu Activation fuction.\n",
    "3. A fully connected output layer.\n",
    "\n",
    "\n",
    "Note: select padding of the convolution input such that output height and width are an integer division of input size. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        \n",
    "        # Define the layers/parameters here\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        # implement the forward propagation\n",
    "        \n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CNN()\n",
    "\n",
    "# if you use GPU, uncomment the floolowing line\n",
    "# cnn_model = cnn_model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Do forward propagation for a minibatch and verify that the output shape equals the number of samples in a minibatch by the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,labels = next(iter(train_loader))\n",
    "\n",
    "output = cnn_model(data)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Set up the loss and optimizer. 8$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = None\n",
    "\n",
    "# Learning rate\n",
    "lr = None\n",
    "epochs = None\n",
    "# Modify this op\n",
    "optimizer = torch.optim.SGD(None, None, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Hyperparamter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Do grid search to find a good combination of the optimizer hyperparamters. Use \n",
    "1. lr=1e-3, 1e-2, 1e-1, 1e-0, 10\n",
    "1. momentum = 0.85, 0.9, 0.95, 0.99\n",
    "2. weight decay = 0, 1e-4, 1e-2\n",
    "\n",
    "You need to use nested loop. Run each experiment for two epochs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Plot the training loss. Label each line with corresponding parameters (lr, momentum, and weight decay).  Pick the set of parameters that yields the best loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Use the hyperparamters selected in the previous stage to train the model for 50 epochs. Track validation accuracy and save the model whenever validation accuracy is higher. Follow the template and instructions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_avg_loss = []\n",
    "val_avg_loss = []\n",
    "best_val_acc = 0.0\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    \n",
    "    # Setting the model to training mode\n",
    "    cnn_model.train()\n",
    "    \n",
    "    for batch in iter(train_loader):\n",
    "        # Setting the gradients to zero. Otherwise, the gradients will be accumulated over iterations\n",
    "        cnn_model.zero_grad()\n",
    "        # unpack batch to get data and associate labels\n",
    "        data, target_labels = batch\n",
    "        # if you are using GPU, uncomment the following line\n",
    "#         data, target_labels = data.cuda(), target_labels.cuda()\n",
    "        \n",
    "        # forward propagte\n",
    "        prediction = cnn_model(data)\n",
    "        # calculating loss\n",
    "        loss = loss_fn(prediction, target_labels)\n",
    "        \n",
    "        \n",
    "        # back propagation from loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # write code to calculate average training loss\n",
    "\n",
    "        # write code to calculate training accuracy\n",
    "        \n",
    "        ###### Evaluation ####\n",
    "        \n",
    "        # Note, evaluation here is done after each epoch. \n",
    "        \n",
    "    # During validatioin, we do not optimize so no need to build computation graph for gradient. \n",
    "    # Hence, turning off gradients using torch.no_grad()\n",
    "    with torch.no_grad():\n",
    "        cnn_model.eval()\n",
    "        for batch in iter(val_loader):\n",
    "            # unpack batch to get data and associate labels\n",
    "            data, target_labels = batch\n",
    "            # forward propagte\n",
    "            prediction = cnn_model(data)\n",
    "            # calculating loss\n",
    "            loss = loss_fn(prediction, target_labels)\n",
    "\n",
    "        # write code to calculate average validation loss\n",
    "\n",
    "        # write code to calculate validation accuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Plot losses vs epoch for training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Load the saved best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Use the loaded model to calculate the accuracy for the test data. Show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Use transfer learning to finetune a pretrained ``ResNet18`` model. \n",
    "\n",
    "1. Load the pretrained model\n",
    "2. Get the transform for training and validation.\n",
    "3. Define the dataset for training, validation, and test again with the new transform.\n",
    "4. Define dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) \n",
    "1. Set the ``requires_grad`` to ``False`` for all parameters of the network.\n",
    "2. Replace the last layer to adpat to your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Train and evaluate with a suitable set of hyperparameters and save the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) evaluate on the test data and report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
