#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®é›†å‡†å¤‡è„šæœ¬
åŠŸèƒ½ï¼š
1. éªŒè¯COCOæ ‡æ³¨æ–‡ä»¶
2. åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†
3. è°ƒæ•´å›¾ç‰‡å¤§å°åˆ°ç›®æ ‡åˆ†è¾¨ç‡
4. ç”Ÿæˆæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
"""

import sys
import json
import shutil
from pathlib import Path
from PIL import Image
import numpy as np
from collections import defaultdict
import random

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import config

def load_coco_annotations(annotation_file):
    """åŠ è½½COCOæ ‡æ³¨æ–‡ä»¶"""
    print(f"\nğŸ“‚ Loading COCO annotations from: {annotation_file}")

    if not annotation_file.exists():
        print(f"âŒ Error: Annotation file not found: {annotation_file}")
        print(f"   Please export your CVAT annotations to: {annotation_file}")
        sys.exit(1)

    with open(annotation_file, 'r', encoding='utf-8') as f:
        coco_data = json.load(f)

    print(f"âœ“ Loaded annotations successfully")
    return coco_data

def validate_coco_data(coco_data):
    """éªŒè¯COCOæ•°æ®æ ¼å¼"""
    print("\nğŸ” Validating COCO data...")

    errors = []

    # æ£€æŸ¥å¿…éœ€çš„å­—æ®µ
    required_fields = ['images', 'annotations', 'categories']
    for field in required_fields:
        if field not in coco_data:
            errors.append(f"Missing required field: {field}")

    if errors:
        print("âŒ Validation failed:")
        for error in errors:
            print(f"   - {error}")
        return False

    # ç»Ÿè®¡ä¿¡æ¯
    num_images = len(coco_data['images'])
    num_annotations = len(coco_data['annotations'])
    num_categories = len(coco_data['categories'])

    print(f"âœ“ Validation passed")
    print(f"   - Images: {num_images}")
    print(f"   - Annotations: {num_annotations}")
    print(f"   - Categories: {num_categories}")

    # æ£€æŸ¥ç±»åˆ«åç§°æ˜¯å¦åŒ¹é…é…ç½®
    category_names = {cat['id']: cat['name'] for cat in coco_data['categories']}
    config_classes = set(config.CLASSES)
    coco_classes = set(category_names.values())

    if config_classes != coco_classes:
        print(f"\nâš ï¸  Warning: Class mismatch detected!")
        print(f"   Config classes: {config_classes}")
        print(f"   COCO classes: {coco_classes}")
        print(f"   Missing in COCO: {config_classes - coco_classes}")
        print(f"   Extra in COCO: {coco_classes - config_classes}")

    # æ¯ä¸ªç±»åˆ«çš„å®ä¾‹æ•°é‡
    category_counts = defaultdict(int)
    for ann in coco_data['annotations']:
        cat_id = ann['category_id']
        cat_name = category_names.get(cat_id, f"Unknown-{cat_id}")
        category_counts[cat_name] += 1

    print(f"\nğŸ“Š Instances per category:")
    for cat_name, count in sorted(category_counts.items()):
        print(f"   - {cat_name}: {count}")

    return True

def split_dataset(coco_data, train_ratio, val_ratio, test_ratio, seed=42):
    """åˆ’åˆ†æ•°æ®é›†"""
    print(f"\nâœ‚ï¸  Splitting dataset: Train {train_ratio*100:.0f}% | Val {val_ratio*100:.0f}% | Test {test_ratio*100:.0f}%")

    # è®¾ç½®éšæœºç§å­
    random.seed(seed)
    np.random.seed(seed)

    # è·å–æ‰€æœ‰å›¾åƒID
    image_ids = [img['id'] for img in coco_data['images']]
    random.shuffle(image_ids)

    # è®¡ç®—åˆ’åˆ†ç‚¹
    num_images = len(image_ids)
    train_end = int(num_images * train_ratio)
    val_end = train_end + int(num_images * val_ratio)

    # åˆ’åˆ†
    train_ids = set(image_ids[:train_end])
    val_ids = set(image_ids[train_end:val_end])
    test_ids = set(image_ids[val_end:])

    print(f"âœ“ Split complete:")
    print(f"   - Train: {len(train_ids)} images")
    print(f"   - Val: {len(val_ids)} images")
    print(f"   - Test: {len(test_ids)} images")

    return train_ids, val_ids, test_ids

def create_split_coco(coco_data, image_ids, split_name):
    """ä¸ºç‰¹å®šåˆ’åˆ†åˆ›å»ºCOCOæ•°æ®"""
    split_data = {
        'images': [],
        'annotations': [],
        'categories': coco_data['categories'],
        'info': coco_data.get('info', {}),
        'licenses': coco_data.get('licenses', [])
    }

    # ç­›é€‰å›¾åƒ
    image_id_to_data = {img['id']: img for img in coco_data['images']}
    for img_id in image_ids:
        if img_id in image_id_to_data:
            split_data['images'].append(image_id_to_data[img_id])

    # ç­›é€‰æ ‡æ³¨
    for ann in coco_data['annotations']:
        if ann['image_id'] in image_ids:
            split_data['annotations'].append(ann)

    return split_data

def resize_and_copy_images(coco_data, image_ids, source_dir, target_dir, target_size):
    """è°ƒæ•´å›¾ç‰‡å¤§å°å¹¶å¤åˆ¶åˆ°ç›®æ ‡ç›®å½•"""
    print(f"\nğŸ–¼ï¸  Processing images for {target_dir.name}...")

    target_dir.mkdir(parents=True, exist_ok=True)

    image_id_to_file = {img['id']: img['file_name'] for img in coco_data['images']}

    success_count = 0
    error_count = 0

    for i, img_id in enumerate(image_ids, 1):
        if img_id not in image_id_to_file:
            print(f"âš ï¸  Warning: Image ID {img_id} not found in annotations")
            continue

        filename = image_id_to_file[img_id]
        source_path = source_dir / filename

        if not source_path.exists():
            print(f"âŒ Error: Image not found: {source_path}")
            error_count += 1
            continue

        try:
            # æ‰“å¼€å›¾ç‰‡
            img = Image.open(source_path)

            # è½¬æ¢ä¸ºRGBï¼ˆå¤„ç†ç°åº¦å›¾å’ŒRGBAï¼‰
            if img.mode != 'RGB':
                img = img.convert('RGB')

            # ä¿æŒå®½é«˜æ¯”ç¼©æ”¾
            img.thumbnail((target_size, target_size), Image.Resampling.LANCZOS)

            # ä¿å­˜
            target_path = target_dir / filename
            img.save(target_path, quality=95, optimize=True)

            success_count += 1

            if i % 20 == 0:
                print(f"   Processed: {i}/{len(image_ids)}")

        except Exception as e:
            print(f"âŒ Error processing {filename}: {e}")
            error_count += 1

    print(f"âœ“ Processing complete: {success_count} succeeded, {error_count} failed")

    return success_count, error_count

def generate_dataset_info(coco_data, split_info):
    """ç”Ÿæˆæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
    print("\nğŸ“Š Generating dataset statistics...")

    info = {
        'total_images': len(coco_data['images']),
        'total_annotations': len(coco_data['annotations']),
        'num_classes': len(coco_data['categories']),
        'classes': [cat['name'] for cat in coco_data['categories']],
        'splits': split_info,
        'config': {
            'image_size': config.IMAGE_SIZE,
            'train_ratio': config.TRAIN_RATIO,
            'val_ratio': config.VAL_RATIO,
            'test_ratio': config.TEST_RATIO,
            'random_seed': config.RANDOM_SEED
        }
    }

    # æ¯ä¸ªç±»åˆ«çš„ç»Ÿè®¡
    category_stats = defaultdict(lambda: {
        'total': 0,
        'train': 0,
        'val': 0,
        'test': 0
    })

    category_names = {cat['id']: cat['name'] for cat in coco_data['categories']}

    for ann in coco_data['annotations']:
        cat_id = ann['category_id']
        cat_name = category_names.get(cat_id, f"Unknown-{cat_id}")
        img_id = ann['image_id']

        category_stats[cat_name]['total'] += 1

        if img_id in split_info['train_ids']:
            category_stats[cat_name]['train'] += 1
        elif img_id in split_info['val_ids']:
            category_stats[cat_name]['val'] += 1
        elif img_id in split_info['test_ids']:
            category_stats[cat_name]['test'] += 1

    info['category_stats'] = dict(category_stats)

    return info

def save_dataset_info(info, output_file):
    """ä¿å­˜æ•°æ®é›†ä¿¡æ¯"""
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(info, f, indent=2, ensure_ascii=False)

    print(f"âœ“ Dataset info saved to: {output_file}")

def print_dataset_summary(info):
    """æ‰“å°æ•°æ®é›†æ‘˜è¦"""
    print("\n" + "="*60)
    print(" Dataset Summary")
    print("="*60)
    print(f"\nğŸ“Š Overall Statistics:")
    print(f"   - Total Images: {info['total_images']}")
    print(f"   - Total Annotations: {info['total_annotations']}")
    print(f"   - Number of Classes: {info['num_classes']}")

    print(f"\nğŸ“‚ Dataset Splits:")
    print(f"   - Train: {info['splits']['train']} images")
    print(f"   - Val: {info['splits']['val']} images")
    print(f"   - Test: {info['splits']['test']} images")

    print(f"\nğŸ·ï¸  Category Statistics:")
    for cat_name, stats in info['category_stats'].items():
        print(f"   {cat_name}:")
        print(f"      Total: {stats['total']} | Train: {stats['train']} | Val: {stats['val']} | Test: {stats['test']}")

    print("="*60 + "\n")

def main():
    print("\n" + "="*60)
    print(" YOLACT++ Dataset Preparation")
    print("="*60)

    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    config.create_dirs()

    # 1. åŠ è½½COCOæ ‡æ³¨
    coco_data = load_coco_annotations(config.COCO_ANNOTATION_FILE)

    # 2. éªŒè¯æ•°æ®
    if not validate_coco_data(coco_data):
        print("\nâŒ Dataset validation failed. Please fix the errors and try again.")
        sys.exit(1)

    # 3. åˆ’åˆ†æ•°æ®é›†
    train_ids, val_ids, test_ids = split_dataset(
        coco_data,
        config.TRAIN_RATIO,
        config.VAL_RATIO,
        config.TEST_RATIO,
        config.RANDOM_SEED
    )

    # 4. åˆ›å»ºå„ä¸ªåˆ’åˆ†çš„COCOæ–‡ä»¶
    print("\nğŸ“ Creating split annotation files...")

    train_coco = create_split_coco(coco_data, train_ids, 'train')
    val_coco = create_split_coco(coco_data, val_ids, 'val')
    test_coco = create_split_coco(coco_data, test_ids, 'test')

    # ä¿å­˜åˆ’åˆ†åçš„æ ‡æ³¨æ–‡ä»¶
    train_ann_file = config.PROCESSED_DATA_DIR / "train" / "annotations.json"
    val_ann_file = config.PROCESSED_DATA_DIR / "val" / "annotations.json"
    test_ann_file = config.PROCESSED_DATA_DIR / "test" / "annotations.json"

    with open(train_ann_file, 'w', encoding='utf-8') as f:
        json.dump(train_coco, f, indent=2, ensure_ascii=False)

    with open(val_ann_file, 'w', encoding='utf-8') as f:
        json.dump(val_coco, f, indent=2, ensure_ascii=False)

    with open(test_ann_file, 'w', encoding='utf-8') as f:
        json.dump(test_coco, f, indent=2, ensure_ascii=False)

    print(f"âœ“ Annotation files saved")

    # 5. å¤„ç†å¹¶å¤åˆ¶å›¾ç‰‡
    print("\nğŸ–¼ï¸  Processing and copying images...")

    resize_and_copy_images(
        coco_data, train_ids,
        config.RAW_IMAGES_DIR,
        config.PROCESSED_DATA_DIR / "train",
        config.IMAGE_SIZE
    )

    resize_and_copy_images(
        coco_data, val_ids,
        config.RAW_IMAGES_DIR,
        config.PROCESSED_DATA_DIR / "val",
        config.IMAGE_SIZE
    )

    resize_and_copy_images(
        coco_data, test_ids,
        config.RAW_IMAGES_DIR,
        config.PROCESSED_DATA_DIR / "test",
        config.IMAGE_SIZE
    )

    # 6. ç”Ÿæˆæ•°æ®é›†ä¿¡æ¯
    split_info = {
        'train': len(train_ids),
        'val': len(val_ids),
        'test': len(test_ids),
        'train_ids': list(train_ids),
        'val_ids': list(val_ids),
        'test_ids': list(test_ids)
    }

    dataset_info = generate_dataset_info(coco_data, split_info)

    # ä¿å­˜æ•°æ®é›†ä¿¡æ¯
    info_file = config.DATA_ROOT / "dataset_info.json"
    save_dataset_info(dataset_info, info_file)

    # æ‰“å°æ‘˜è¦
    print_dataset_summary(dataset_info)

    print("âœ… Dataset preparation completed successfully!")
    print(f"\nğŸ“ Processed data location: {config.PROCESSED_DATA_DIR}")
    print(f"ğŸ“„ Dataset info: {info_file}")
    print("\nğŸš€ Next step: Run 'python scripts/train.py' to start training")

if __name__ == "__main__":
    main()
